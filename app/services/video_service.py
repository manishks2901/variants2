from celery import Celery
from datetime import datetime
import os
import logging
import uuid
import sys
import time
from typing import Optional, Callable
from decouple import config
import pymongo
import asyncio

from .ffmpeg_service import FFmpegTransformationService
from .s3_service import S3Service
from .punchline_service import EnhancedVideoPunchlineGenerator
from ..database import get_database, connect_to_mongo
from ..models import JobStatus

# Configure logging for Celery workers with enhanced terminal output
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # Terminal output
        logging.FileHandler('celery_worker.log')  # File output
    ],
    force=True  # Override any existing logging configuration
)

# Also configure the root logger to ensure all messages are captured
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# Get logger for this module
logger = logging.getLogger(__name__)

# Ensure transformation logs are visible by setting specific logger levels
ffmpeg_logger = logging.getLogger('app.services.ffmpeg_service')
ffmpeg_logger.setLevel(logging.INFO)

# Initialize Celery
celery_app = Celery(
    'video_processing',
    broker=config('CELERY_BROKER_URL', default='redis://localhost:6379/0'),
    backend=config('CELERY_RESULT_BACKEND', default='redis://localhost:6379/0'),
    include=['app.services.video_service']
)

# Celery configuration with configurable timeouts
TASK_TIMEOUT = int(config('CELERY_TASK_TIMEOUT', default='7200'))  # 2 hours default
SOFT_TIMEOUT = int(config('CELERY_SOFT_TIMEOUT', default='6600'))  # 110 minutes default

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_send_sent_event=True,  # Enable task sent events
    worker_send_task_events=True,  # Enable task events from worker
    task_time_limit=TASK_TIMEOUT,  # Configurable timeout (default: 2 hours)
    task_soft_time_limit=SOFT_TIMEOUT,  # Configurable soft timeout (default: 110 minutes)
    worker_prefetch_multiplier=1,
    result_expires=3600,  # 1 hour
    worker_log_format='[%(asctime)s: %(levelname)s/%(processName)s] %(message)s',
    worker_task_log_format='[%(asctime)s: %(levelname)s/%(processName)s][%(task_name)s(%(task_id)s)] %(message)s',
)

# Task signal handlers for better logging
from celery.signals import task_prerun, task_postrun, task_failure

@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kw):
    logger.info(f"üöÄ Starting task {sender.name} with ID {task_id}")

@task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kw):
    logger.info(f"‚úÖ Completed task {sender.name} with ID {task_id} - State: {state}")

@task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kw):
    logger.error(f"‚ùå Task {sender.name} with ID {task_id} failed: {exception}")

def run_async_in_worker(coro):
    """Safely run async coroutine in Celery worker"""
    try:
        # Try to get existing event loop
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is running, we need to use a thread executor
            import concurrent.futures
            import threading
            
            result = None
            exception = None
            
            def run_in_thread():
                nonlocal result, exception
                try:
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    result = new_loop.run_until_complete(coro)
                    new_loop.close()
                except Exception as e:
                    exception = e
            
            thread = threading.Thread(target=run_in_thread)
            thread.start()
            thread.join()
            
            if exception:
                raise exception
            return result
        else:
            # No running loop, can use asyncio.run
            return asyncio.run(coro)
    except RuntimeError:
        # No event loop, create new one
        return asyncio.run(coro)

def get_sync_database():
    """Get synchronous database connection for Celery workers"""
    try:
        mongodb_uri = config("MONGODB_URI")
        client = pymongo.MongoClient(mongodb_uri)
        db = client.get_database("video-variants")
        return db
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB (sync): {e}")
        return None

async def ensure_database_connection():
    """Ensure database connection is established"""
    db = get_database()
    if db is None:
        await connect_to_mongo()
        db = get_database()
    return db

class VideoProcessingService:
    
    def __init__(self):
        self.s3_service = S3Service()
        self.temp_dir = config('TEMP_DIR', default='/tmp/video-variants')
        os.makedirs(self.temp_dir, exist_ok=True)
    
    async def start_processing_task(self, job_id: str, variants_count: int = 1, strategy: str = "enhanced_metrics"):
        """Start asynchronous video processing task for multiple variants"""
        try:
            # Send task to Celery worker with strategy parameter
            task = process_video_task.delay(job_id, variants_count, strategy)
            
            # Update job with task ID
            db = get_database()
            await db.jobs.update_one(
                {"id": job_id},
                {"$set": {"celery_task_id": task.id}}
            )
            
            logger.info(f"Started processing task for job {job_id} with {variants_count} variant(s) using {strategy} strategy, task ID {task.id}")
            
        except Exception as e:
            logger.error(f"Failed to start processing task for job {job_id}: {e}")
            # Update job status to failed
            db = get_database()
            await db.jobs.update_one(
                {"id": job_id},
                {"$set": {
                    "status": JobStatus.FAILED,
                    "error_message": f"Failed to start processing: {str(e)}"
                }}
            )
            raise

@celery_app.task(bind=True)
def process_video_task(self, job_id: str, variants_count: int = 1, strategy: str = "enhanced_metrics"):
    """
    Celery task for processing multiple video variants - uses sync operations
    """
    try:
        return _process_video_sync(self, job_id, variants_count, strategy)
    except Exception as e:
        logging.error(f"Task failed for job {job_id}: {e}")
        raise

def _process_video_sync(task, job_id: str, variants_count: int = 1, strategy: str = "standard"):
    """
    Synchronous function to process multiple video variants
    """
    start_time = time.time()
    
    # Get synchronous database connection
    db = get_sync_database()
    if db is None:
        raise Exception("Could not connect to database")
    
    s3_service = S3Service()
    temp_dir = config('TEMP_DIR', default='/tmp/video-variants')
    
    # Log processing start with timeout info
    logging.info(f"üöÄ Starting video processing for job {job_id}")
    logging.info(f"   üìä Variants requested: {variants_count}")
    logging.info(f"   üéØ Strategy: {strategy}")
    logging.info(f"   ‚è∞ Task timeout: {TASK_TIMEOUT}s ({TASK_TIMEOUT/60:.1f} minutes)")
    logging.info(f"   ‚ö†Ô∏è Soft timeout: {SOFT_TIMEOUT}s ({SOFT_TIMEOUT/60:.1f} minutes)")
    
    try:
        # Update job status to processing
        db.jobs.update_one(
            {"id": job_id},
            {"$set": {
                "status": JobStatus.PROCESSING,
                "started_at": datetime.utcnow(),
                "progress": 0.0
            }}
        )
        
        # Get job details
        job = db.jobs.find_one({"id": job_id})
        if not job:
            raise Exception(f"Job {job_id} not found")
        
        # Create progress callback
        def update_progress(progress: float):
            db.jobs.update_one(
                {"id": job_id},
                {"$set": {"progress": progress}}
            )
            # Update Celery task progress
            task.update_state(state='PROGRESS', meta={'progress': progress})
        
        update_progress(5.0)  # Starting
        
        # Download input file from S3
        input_filename = f"input_{job_id}_{job['original_filename']}"
        local_input_path = os.path.join(temp_dir, input_filename)
        
        logging.info(f"Downloading input file for job {job_id}")
        # Use sync version for Celery workers
        s3_service.download_file_sync(job['s3_input_key'], local_input_path)
        update_progress(10.0)  # Downloaded
        
        # Get video information using sync version
        video_info = FFmpegTransformationService.get_video_info_sync(local_input_path)
        db.jobs.update_one(
            {"id": job_id},
            {"$set": {
                "video_info": video_info,
                "duration": video_info.get('duration')
            }}
        )
        update_progress(15.0)  # Video info retrieved
        
        # Process multiple variants
        variants = []
        base_progress = 15.0
        progress_per_variant = 75.0 / variants_count  # 15% to 90% for processing
        
        for variant_index in range(variants_count):
            variant_id = str(uuid.uuid4())
            unique_variant_seed = f"{job_id}_variant_{variant_index + 1}"
            
            logging.info(f"üé¨ ===== PROCESSING VARIANT {variant_index + 1}/{variants_count} =====")
            logging.info(f"üìã Job ID: {job_id}")
            logging.info(f"üéØ Variant ID: {variant_id}")
            logging.info(f"üå± Unique Seed: {unique_variant_seed}")
            logging.info(f"üé≤ This variant will get COMPLETELY RANDOM transformations!")
            logging.info(f"===============================================")
            
            # Create variant-specific output filename
            base_name = os.path.splitext(job['original_filename'])[0]
            extension = os.path.splitext(job['original_filename'])[1]
            variant_filename = f"variant_{variant_index + 1}_{base_name}{extension}"
            s3_output_key = f"output/{job_id}/{variant_filename}"
            local_output_path = os.path.join(temp_dir, f"output_{variant_id}_{variant_filename}")
            
            # Create progress callback for this variant
            variant_start_progress = base_progress + (variant_index * progress_per_variant)
            variant_end_progress = base_progress + ((variant_index + 1) * progress_per_variant)
            
            def variant_progress(progress: float):
                mapped_progress = variant_start_progress + (progress * (progress_per_variant / 100))
                update_progress(mapped_progress)
            
            try:
                # Check remaining time before starting variant processing
                elapsed_time = time.time() - start_time
                remaining_time = SOFT_TIMEOUT - elapsed_time
                
                if remaining_time < 300:  # Less than 5 minutes left
                    logging.warning(f"‚ö†Ô∏è Low time remaining for variant {variant_index + 1}: {remaining_time:.1f}s")
                    if remaining_time < 60:  # Less than 1 minute
                        logging.error(f"‚ùå Insufficient time remaining ({remaining_time:.1f}s), skipping variant {variant_index + 1}")
                        continue
                
                logging.info(f"‚è∞ Starting variant {variant_index + 1} - Elapsed: {elapsed_time:.1f}s, Remaining: {remaining_time:.1f}s")
                
                # Apply transformations based on chosen strategy
                if strategy == "seven_layer":
                    logging.info(f"üéØ Using 7-LAYER PIPELINE strategy for variant {variant_index + 1}")
                    applied_transformations = run_async_in_worker(FFmpegTransformationService.apply_seven_layer_transformations(
                        local_input_path,
                        local_output_path,
                        variant_progress,
                        variant_id=f"{job_id}_variant_{variant_index + 1}"
                    ))
                elif strategy == "enhanced_metrics":
                    logging.info(f"üéØ Using ENHANCED METRICS OPTIMIZATION strategy for variant {variant_index + 1}")
                    applied_transformations = run_async_in_worker(FFmpegTransformationService.apply_transformations(
                        local_input_path,
                        local_output_path,
                        variant_progress,
                        variant_id=f"{job_id}_variant_{variant_index + 1}",
                        strategy="enhanced_metrics"
                    ))
                else:  # standard strategy
                    logging.info(f"üé≤ Using STANDARD RANDOM strategy for variant {variant_index + 1}")
                    applied_transformations = run_async_in_worker(FFmpegTransformationService.apply_transformations(
                        local_input_path,
                        local_output_path,
                        variant_progress,
                        variant_id=f"{job_id}_variant_{variant_index + 1}"
                    ))
                
                # Log detailed transformation information
                if strategy == "seven_layer":
                    strategy_name = "7-LAYER PIPELINE"
                elif strategy == "enhanced_metrics":
                    strategy_name = "ENHANCED METRICS OPTIMIZATION"
                else:
                    strategy_name = "STANDARD RANDOM"
                
                logging.info(f"üé¨ ===== VARIANT {variant_index + 1} TRANSFORMATION SUMMARY ({strategy_name}) =====")
                logging.info(f"   üéØ Variant Seed: {unique_variant_seed}")
                logging.info(f"   üìä Total transformations applied: {len(applied_transformations)}")
                logging.info(f"   üé≤ Transformations: {applied_transformations}")
                
                # Categorize applied transformations for detailed logging
                all_transformations = FFmpegTransformationService.get_transformations()
                transform_map = {t.name: t for t in all_transformations}
                
                categories_applied = {}
                for transform_name in applied_transformations:
                    if transform_name in transform_map:
                        category = transform_map[transform_name].category
                        if category not in categories_applied:
                            categories_applied[category] = []
                        categories_applied[category].append(transform_name)
                    else:
                        # Handle transformations not in the map
                        if 'other' not in categories_applied:
                            categories_applied['other'] = []
                        categories_applied['other'].append(transform_name)
                
                # Log by category with emojis
                category_emojis = {
                    'visual': 'üé®',
                    'audio': 'üéµ', 
                    'structural': 'üèóÔ∏è',
                    'metadata': 'üìã',
                    'semantic': 'üß†',
                    'advanced': '‚ö°',
                    'enhanced': '‚≠ê',
                    'instagram': 'üì±',
                    'other': 'üîß'
                }
                
                for category, transforms in categories_applied.items():
                    emoji = category_emojis.get(category, 'üîß')
                    logging.info(f"   {emoji} {category.upper()} ({len(transforms)}): {', '.join(transforms)}")
                
                # Log specific transformation details from CSV
                transformation_details = []
                for transform_name in applied_transformations:
                    if 'frequency_band_shifting' in transform_name:
                        transformation_details.append("üéµ Frequency Band Shifting: -500Hz to +500Hz")
                    elif 'multi_band_eq' in transform_name:
                        transformation_details.append("üéõÔ∏è Multi-band EQ: Low/Mid/High Gain: -6 to +6dB")
                    elif 'harmonic_distortion' in transform_name:
                        transformation_details.append("üîä Harmonic Distortion: 10% to 20% intensity")
                    elif 'stereo_phase_inversion' in transform_name:
                        transformation_details.append("üîÑ Stereo Phase Inversion: Right channel inverted")
                    elif 'stereo_width_manipulation' in transform_name:
                        transformation_details.append("‚ÜîÔ∏è Stereo Width: 0.5x to 2.0x manipulation")
                    elif 'binaural_processing' in transform_name:
                        transformation_details.append("üß† Binaural Processing: Low frequency pulsation")
                    elif 'echo_delay_variation' in transform_name:
                        transformation_details.append("üîä Echo: 100ms-500ms delay, 20%-60% decay")
                    elif 'audio_chorus_effect' in transform_name:
                        transformation_details.append("üéº Chorus: 2ms-8ms delay, 0.1-0.3 depth")
                    elif 'dynamic_range_compression' in transform_name:
                        transformation_details.append("üóúÔ∏è Compression: -20 to -10dB threshold, 2-8 ratio")
                    elif 'instagram_speed_micro_changes' in transform_name:
                        transformation_details.append("üì± Instagram Speed: 0.97x-1.03x every 8-12s")
                    elif 'instagram_pitch_shift_segments' in transform_name:
                        transformation_details.append("üì± Instagram Pitch: ¬±2%-5% every 10-15s")
                    elif 'variable_frame_interpolation' in transform_name:
                        transformation_details.append("üé¨ Frame Interpolation: [0.9,0.95,1.05,1.1]x speeds")
                    elif 'instagram_rotation_micro' in transform_name:
                        transformation_details.append("üì± Instagram Rotation: ¬±0.5¬∞-2¬∞ every 10-15s")
                    elif 'instagram_crop_resize_cycle' in transform_name:
                        transformation_details.append("üì± Instagram Crop-Resize: 2px-20px crop then resize")
                    elif 'color_channel_swapping' in transform_name:
                        transformation_details.append("üé® Color Channel Swapping: Random RGB swaps")
                    elif 'chromatic_aberration' in transform_name:
                        transformation_details.append("üî¥üîµ Chromatic Aberration: ¬±3px red/blue shift")
                    elif 'perspective_distortion' in transform_name:
                        transformation_details.append("üìê Perspective: ¬±5px keystone adjustments")
                    elif 'barrel_distortion' in transform_name:
                        transformation_details.append("ü•Ω Barrel Distortion: ¬±0.1 lens coefficient")
                    elif 'optical_flow_stabilization' in transform_name:
                        transformation_details.append("üìπ Stabilization: Shakiness 2-8")
                    elif 'film_grain_simulation' in transform_name:
                        transformation_details.append("üéûÔ∏è Film Grain: 0.1-0.3 noise intensity")
                    elif 'texture_blend_overlay' in transform_name:
                        transformation_details.append("üñºÔ∏è Texture Overlay: 0.1-0.3 opacity")
                    elif 'particle_overlay_system' in transform_name:
                        transformation_details.append("‚ú® Particle System: 10-30 count, 0.1-0.3 opacity")
                    elif 'advanced_metadata_spoofing' in transform_name:
                        transformation_details.append("üìã Metadata Spoofing: Random camera/software/timestamps")
                    elif 'gps_exif_randomization' in transform_name:
                        transformation_details.append("üåç GPS Randomization: ¬±90¬∞ lat, ¬±180¬∞ lon")
                    elif 'camera_settings_simulation' in transform_name:
                        transformation_details.append("üì∑ Camera Settings: ISO 100-6400, f/1.4-8, 1/30-500s")
                    elif 'codec_parameter_variation' in transform_name:
                        transformation_details.append("üé¨ Codec Variation: CRF 18-26, various presets")
                    elif 'uuid_injection_system' in transform_name:
                        transformation_details.append("üîë UUID Injection: Unique v4 UUID")
                
                if transformation_details:
                    logging.info(f"   üìù TRANSFORMATION DETAILS:")
                    for detail in transformation_details[:10]:  # Limit to first 10 to avoid log spam
                        logging.info(f"      {detail}")
                    if len(transformation_details) > 10:
                        logging.info(f"      ... and {len(transformation_details) - 10} more transformations")
                
                # Log transformation effectiveness metrics
                logging.info(f"   üìä EFFECTIVENESS METRICS:")
                logging.info(f"      üéØ Visual transformations: {len(categories_applied.get('visual', []))} (pHash/SSIM disruption)")
                logging.info(f"      üéµ Audio transformations: {len(categories_applied.get('audio', []))} (fingerprint breaking)")
                logging.info(f"      üì± Instagram optimizations: {len(categories_applied.get('instagram', []))} (copyright bypass)")
                logging.info(f"      üìã Metadata randomization: {len(categories_applied.get('metadata', []))} (EXIF spoofing)")
                logging.info(f"      ‚≠ê Enhanced features: {len(categories_applied.get('enhanced', []))} (advanced effects)")
                
                # Calculate estimated bypass confidence
                total_score = 0
                if 'visual' in categories_applied:
                    total_score += len(categories_applied['visual']) * 15  # Visual has high impact
                if 'audio' in categories_applied:
                    total_score += len(categories_applied['audio']) * 12   # Audio fingerprint breaking
                if 'instagram' in categories_applied:
                    total_score += len(categories_applied['instagram']) * 18  # Instagram-specific optimizations
                if 'metadata' in categories_applied:
                    total_score += len(categories_applied['metadata']) * 8   # Metadata helps but less impact
                if 'enhanced' in categories_applied:
                    total_score += len(categories_applied['enhanced']) * 10  # Enhanced features
                
                bypass_confidence = min(95, max(60, total_score))  # Cap between 60-95%
                logging.info(f"      üõ°Ô∏è Estimated bypass confidence: {bypass_confidence}%")
                
                logging.info(f"   ‚úÖ Variant {variant_index + 1} processing completed successfully")
                
                # Upload processed variant to S3
                logging.info(f"Uploading variant {variant_index + 1} for job {job_id}")
                s3_output_url = s3_service.upload_processed_file_sync(
                    local_output_path, 
                    s3_output_key
                )
                
                # Get output file size
                output_file_size = os.path.getsize(local_output_path)
                
                # Create variant record
                variant = {
                    "variant_id": variant_id,
                    "s3_output_key": s3_output_key,
                    "applied_transformations": applied_transformations,
                    "transformation_count": len(applied_transformations),
                    "file_size": output_file_size
                }
                
                variants.append(variant)
                
                # Update variants completed count
                db.jobs.update_one(
                    {"id": job_id},
                    {"$set": {"variants_completed": len(variants)}}
                )
                
                # Clean up local variant file
                if os.path.exists(local_output_path):
                    os.remove(local_output_path)
                
                logging.info(f"‚úÖ Completed variant {variant_index + 1}/{variants_count} with {len(applied_transformations)} UNIQUE transformations")
                logging.info(f"=================================================")
                
            except Exception as variant_error:
                logging.error(f"Failed to process variant {variant_index + 1} for job {job_id}: {variant_error}")
                # Continue with other variants instead of failing the entire job
                continue
        
        update_progress(90.0)  # All variants processed
        
        # Check if we have at least one successful variant
        if not variants:
            raise Exception("No variants were successfully processed")
        
        # ===== FINAL SUMMARY OF ALL VARIANTS =====
        total_time = time.time() - start_time
        logging.info(f"üéâ ===== FINAL VARIANTS SUMMARY =====")
        logging.info(f"üìã Job ID: {job_id}")
        logging.info(f"üé¨ Total variants created: {len(variants)}/{variants_count}")
        logging.info(f"‚è∞ Total processing time: {total_time:.2f}s ({total_time/60:.1f} minutes)")
        logging.info(f"‚ö° Average time per variant: {total_time/len(variants):.2f}s")
        
        # Show transformation summary for all variants
        all_transformations_used = set()
        for i, variant in enumerate(variants):
            if 'transformations_applied' in variant:
                transforms = variant['transformations_applied']
                all_transformations_used.update(transforms)
                logging.info(f"   üéØ Variant {i+1}: {len(transforms)} transformations -> {transforms[:3]}...")  # Show first 3
        
        logging.info(f"üé≤ Total unique transformations used across all variants: {len(all_transformations_used)}")
        logging.info(f"‚ú® Each variant is COMPLETELY UNIQUE with different random transformations!")
        logging.info(f"=====================================")
        
        # Update job as completed
        db.jobs.update_one(
            {"id": job_id},
            {"$set": {
                "status": JobStatus.COMPLETED,
                "progress": 100.0,
                "completed_at": datetime.utcnow(),
                "variants": variants,
                "variants_completed": len(variants)
            }}
        )
        
        logging.info(f"Job {job_id} completed successfully with {len(variants)}/{variants_count} variants")
        
        # Clean up input file
        try:
            if os.path.exists(local_input_path):
                os.remove(local_input_path)
        except Exception as cleanup_error:
            logging.warning(f"Failed to clean up input file for job {job_id}: {cleanup_error}")
        
        return {
            "job_id": job_id,
            "status": "completed",
            "variants_requested": variants_count,
            "variants_completed": len(variants),
            "variants": variants
        }
        
    except Exception as e:
        logging.error(f"Processing failed for job {job_id}: {e}")
        
        # Update job as failed
        db.jobs.update_one(
            {"id": job_id},
            {"$set": {
                "status": JobStatus.FAILED,
                "error_message": str(e),
                "completed_at": datetime.utcnow()
            }}
        )
        
        # Clean up any local files
        try:
            input_filename = f"input_{job_id}_{job.get('original_filename', 'unknown')}"
            local_input_path = os.path.join(temp_dir, input_filename)
            
            if os.path.exists(local_input_path):
                os.remove(local_input_path)
                
            # Clean up any partial variant files
            for file in os.listdir(temp_dir):
                if file.startswith(f"output_{job_id}_") or file.startswith(f"variant_{job_id}_"):
                    os.remove(os.path.join(temp_dir, file))
        except:
            pass
        
        raise

# Celery periodic tasks (optional)
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    'cleanup-old-files': {
        'task': 'app.services.video_service.cleanup_old_files',
        'schedule': crontab(hour=2, minute=0),  # Run daily at 2 AM
    },
}

@celery_app.task
def cleanup_old_files():
    """Clean up old temporary files"""
    import time
    temp_dir = config('TEMP_DIR', default='/tmp/video-variants')
    
    try:
        current_time = time.time()
        max_age = 24 * 60 * 60  # 24 hours
        
        for filename in os.listdir(temp_dir):
            file_path = os.path.join(temp_dir, filename)
            if os.path.isfile(file_path):
                file_age = current_time - os.path.getmtime(file_path)
                if file_age > max_age:
                    os.remove(file_path)
                    logging.info(f"Removed old file: {filename}")
                    
        logging.info("Cleanup completed successfully")
        
    except Exception as e:
        logging.error(f"Cleanup failed: {e}")

# Health check task
@celery_app.task
def health_check():
    """Health check task for monitoring"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}
